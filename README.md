# ğŸ§  LLM-Powered Chatbot with RAG on Custom Domain (Local Inference)
<img width="829" height="483" alt="image" src="https://github.com/user-attachments/assets/0e4cf32d-d05b-4ae3-bb85-d38430c11329" />


This project is a fully local Retrieval-Augmented Generation (RAG) chatbot that answers questions over your own documents using a local LLM (`flan-t5-base`). It uses FAISS for semantic search and Streamlit for an interactive frontend.

---

## âœ… Features

- ğŸ” Upload and query your own documents (`.pdf` or `.txt`)
- ğŸ§  Local embedding using `sentence-transformers`
- âš¡ Fast similarity search with FAISS
- ğŸ¤– Answer generation with a local Hugging Face model (`transformers.pipeline`)
- ğŸ’» Streamlit-based user interface
- ğŸ”Œ No OpenAI or Hugging Face APIs required

---

## ğŸ—‚ï¸ Project Structure
rag_chatbot/
â”œâ”€â”€ app.py # Main Streamlit app
â”œâ”€â”€ document_loader.py # Loads and chunks documents
â”œâ”€â”€ vector_store.py # Handles FAISS vector store
â”œâ”€â”€ qa_chain.py # Builds the RetrievalQA chain using local LLM
â”œâ”€â”€ docs/ # Folder for uploaded documents
â”œâ”€â”€ .env # For future API support (optional)
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ assets/
â””â”€â”€ working_demo.png # Screenshot of the app in action



