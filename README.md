# ğŸ§  LLM-Powered Chatbot with RAG on Custom Domain (Local Inference)
<img width="829" height="483" alt="image" src="https://github.com/user-attachments/assets/0e4cf32d-d05b-4ae3-bb85-d38430c11329" />

A fully local Retrieval-Augmented Generation (RAG) chatbot built using LangChain and Hugging Face Transformers. It supports question-answering over custom documents without relying on any external API â€” everything runs locally.

---

## âœ… Features

- Upload `.pdf` and `.txt` files as knowledge base
- Chunk and embed documents using `sentence-transformers`
- Perform semantic search with FAISS
- Answer questions using a local LLM (`flan-t5-base`)
- Interactive web interface built with Streamlit

---

## ğŸ—‚ï¸ Project Structure

```bash
rag_chatbot/
â”œâ”€â”€ app.py                  # Main Streamlit app
â”œâ”€â”€ document_loader.py      # Loads and chunks documents
â”œâ”€â”€ vector_store.py         # Handles FAISS vector store
â”œâ”€â”€ qa_chain.py             # Builds the RetrievalQA chain using local LLM
â”œâ”€â”€ docs/                   # Folder for uploaded documents
â”œâ”€â”€ .env                    # For future API support (optional)
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ assets/
    â””â”€â”€ working_demo.png    # Screenshot of the app in action




